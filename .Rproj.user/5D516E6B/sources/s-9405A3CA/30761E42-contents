print(sentimentAnalysis("@ImAmarilisCas","@karymeibacruz"))

sentimentAnalysis <- function(username,username2){
  
  tema_graf <-
    theme_minimal() +
    theme(text = element_text(family = "serif"),
    panel.grid.minor = element_blank(),
    strip.background = element_rect(fill = "#EBEBEB", colour = NA),
    legend.position = "none",
    legend.box.background = element_rect(fill = "#EBEBEB", colour = NA))
  
  token <- create_token(
    app = "Practica1",
    consumer_key = "uvcMjAH4FJglTG8QS9zlpJC01",
    consumer_secret = "Ed2c2FhXzpHFagbfTVb3gTnziNTQ3hDk0UDRiiraPfupO5At0s",
    access_token = "1442475234913775619-hKJkC4Pp3N0ztkvGAue6niVGtra8YZ",
    access_secret = "QqW0LjrlYr1YrkllT1RhMbi4LmUvARaecltG6HTzAobUf"
  )
  
  datos_new <- get_timeline(user=username,
                            n = 200,
                            parse = TRUE,
                            check = FALSE,
                            include_rts = FALSE)
  datos_new2 <- get_timeline(user=username2,
                             n = 200, 
                             parse = TRUE, 
                             check = TRUE, 
                             include_rts = FALSE)
  
  #datos_new <- map_if(.x=datos_new, .p=is.numeric, .f=as.character)
  
  tweets1 <- bind_rows(datos_new, datos_new2)
  # Mostrar el total de tweets extraídos 
  resumen <- tweets1 %>% group_by("screen_name") %>% summarise(numero_tweets = n()); resumen
  # Seleccionar y renombrar columnas
  tweets1 <- tweets1 %>% select(screen_name, created_at, status_id, text)
  tweets1 <- tweets1 %>% rename(autor = screen_name, fecha = created_at, texto = text, tweet_id = status_id)
  
  tweets1
  
  # tokenización a cada tweet
  tweets1 <- tweets1 %>% mutate(texto_tokenizado = map(.x = texto, .f = limpiar_tokenizar))
  tweets1 %>% select(texto_tokenizado) %>% head()
  
  tweets_tidy <- tweets1 %>% select(-texto) %>% unnest(cols = c(texto_tokenizado))
  tweets_tidy <- tweets_tidy %>% rename(token = texto_tokenizado)
  
  plotTWDate <- ggplot(tweets1, aes(x = as.Date(fecha), fill = autor)) +
    geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +
    scale_x_date(date_labels = "%m-%Y", date_breaks = "2 week") +
    labs(x = "Posting Date", y = "Number of tweets") +
    facet_wrap(~ autor, ncol = 1) + theme_bw() + theme(axis.text.x = element_text(angle = 90))
  
  plotTWDate
  
  tweets_mes_anyo <- tweets1 %>% mutate(mes_anyo = format(fecha, "%Y-%m"))
  tweets_mes_anyo <- tweets_mes_anyo %>% group_by(autor, mes_anyo) %>% summarise(n = n()) %>% ggplot(aes(x = mes_anyo, y = n, color 
                                                                                      = autor)) +
    geom_line(aes(group = autor)) +
    labs(title = "Amount of posted tweets", x = "Posting Date",
         y = "Number of tweets") + theme_bw() +
    theme(axis.text.x = element_text(angle = 90, size = 6),
          legend.position = "bottom")
  
  
  tweets_mes_anyo
  # 3) Conteo de palabras por usuario: 
  tweets_tidy %>% group_by(autor) %>% summarise(n = n()) 
  wordCount <- tweets_tidy %>% ggplot(aes(x = autor)) + geom_bar() + coord_flip() + theme_bw()
  
  tweets_tidy %>% select(autor, token) %>% distinct() %>% group_by(autor) %>% summarise(palabras_distintas = n()) 
  wordCountDist <- tweets_tidy %>% select(autor, token) %>% distinct() %>%
    ggplot(aes(x = autor)) + geom_bar() + coord_flip() + theme_bw()
  
  require(sm)
  
  lista_stopwords <- stopwords("spanish")
  # Se añade el término amp al listado de stopwords
  lista_stopwords <- c(lista_stopwords, "amp")
  lista_stopwords
  
  
  #5) Correlación entre usuarios por palabras utilizadas:
  tweets_spread <- tweets_tidy %>% group_by(autor, token) %>% count(token) %>%
    spread(key = autor, value = n, fill = NA, drop = TRUE)
  
  cor.test(~ JoeBiden + BernieSanders, method = "pearson", data = tweets_spread)
  
  p1 <- ggplot(tweets_spread, aes(JoeBiden, BernieSanders)) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, 
                                                                          height = 0.25) + geom_text(aes(label = token), check_overlap = TRUE, vjust = 1.5) + 
    scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) +
    geom_abline(color = "red") + theme_bw() + theme(axis.text.x = element_blank(),
                                                    axis.text.y = element_blank())
  
  p1
  
  
  # 6) Conteo de palabras comunes entre dos usuarios: 
  palabras_comunes <- dplyr::intersect(tweets_tidy %>% filter(autor=="BernieSanders") %>%
                                         select(token), tweets_tidy %>% filter(autor=="JoeBiden") %>%
                                         select(token)) %>% nrow()
  
  paste("Número de palabras comunes entre Bernie Sanders y Joe Biden", palabras_comunes)
  
  
  # 7) Comparación en el uso de palabras:
  
  # a) Pivotaje y despivotaje
  tweets_spread <- tweets_tidy %>% group_by(autor, token) %>% count(token) %>% spread(key = autor, value = n, fill = 0, drop = TRUE)
  tweets_unpivot <- tweets_spread %>% gather(key = "autor", value = "n", -token)
  
  # b) Selección de los autores Joe Biden y Bernie Sanders
  tweets_unpivot <- tweets_unpivot %>% 
    filter(autor %in% c("JoeBiden", "BernieSanders"))
  
  # c) Se añade el total de palabras de cada autor
  tweets_unpivot <- tweets_unpivot %>% left_join(tweets_tidy %>% group_by(autor) 
                                                 %>%summarise(N = n()), by = "autor")
  tweets_unpivot
  
  # d) Cálculo de odds y log of odds de cada palabra
  tweets_logOdds <- tweets_unpivot %>% mutate(odds = (n + 1) / (N + 1))
  tweets_logOdds <- tweets_logOdds %>% select(autor, token, odds) %>% spread(key = autor, value = odds)
  tweets_logOdds <- tweets_logOdds %>% mutate(log_odds = log(JoeBiden/BernieSanders), abs_log_odds = 
                                                abs(log_odds))
  tweets_logOdds <- tweets_logOdds %>% mutate(autor_frecuente = if_else(log_odds > 0, "@JoeBiden", 
                                                                        "@BernieSanders"))
  tweets_logOdds %>% arrange(desc(abs_log_odds)) %>% head() 
  a <- tweets_logOdds %>% group_by(autor_frecuente) %>% top_n(15, abs_log_odds)
  
  # e) Graficar resultados
  logOddsplot <- ggplot(a,aes(x = reorder(token, log_odds), y = log_odds, fill = autor_frecuente)) + 
    geom_col() + labs(x = "palabra", y = "log odds ratio (JoeBiden / BernieSanders)") +
    coord_flip() + theme_bw()
  
  # f) Representación gráfica de las frecuencias
  tweets_tidy %>% group_by(autor, token) %>% count(token) %>% group_by(autor) %>%
    top_n(10, n) %>% arrange(autor, desc(n)) %>%
    ggplot(aes(x = reorder(token,n), y = n, fill = autor)) +
    geom_col() +
    theme_bw() +
    labs(y = "", x = "") +
    theme(legend.position = "none") +
    coord_flip() +
    facet_wrap(~autor,scales = "free", ncol = 1, drop = TRUE)
  
  
  
  
  
  
  # b) Selección del autor
  tweets_unpivot <- tweets_unpivot %>% 
    filter(autor %in% c("@JoeBiden", "@BernieSanders"))
  
  # c) Se añade el total de palabras del autor
  tweets_unpivot <- tweets_unpivot %>% left_join(tweets_tidy %>% group_by(autor) 
                                                 %>%summarise(N = n()), by = "autor")
  tweets_unpivot
  
  
  # f) Representación gráfica de las frecuencias
  commonWords <- tweets_tidy %>% group_by(autor, token) %>% count(token) %>% group_by(autor) %>%
    top_n(10, n) %>% arrange(autor, desc(n)) %>%
    ggplot(aes(x = reorder(token,n), y = n, fill = autor)) +
    geom_col() +
    theme_bw() +
    labs(y = "", x = "") +
    theme(legend.position = "none") +
    coord_flip() +
    facet_wrap(~autor,scales = "free", ncol = 1, drop = TRUE)
  
  #Clasificación y entrenamiento del modelo
  tweets_user <- tweets1 %>% filter(autor %in% c("JoeBiden", "BernieSanders"))
  set.seed(123)
  train <- sample(x = 1:nrow(tweets_user), size= 0.8*nrow(tweets_user))
  tweets_train <- tweets_user[train, ]
  tweets_test  <- tweets_user[-train, ]
  
  # Es importante verificar que la proporción de 
  # cada grupo es similar en el set de entrenamiento 
  # y en el de test.
  table(tweets_train$autor) / length(tweets_train$autor)
  table(tweets_test$autor) / length(tweets_test$autor)
  
  # Limpieza y tokenización de los documentos de entrenamiento
  tweets_train$texto <- tweets_train$texto %>% map(.f = limpiar_tokenizar) %>% map(.f = paste, collapse = " ") %>% 
    unlist()
  
  # Creación de la matriz documento-término
  matriz_tfidf_train <- dfm(x = tweets_train$texto, remove = lista_stopwords)
  
  # Se reduce la dimensión de la matriz eliminando aquellos 
  # términos que aparecen en menos de 5 documentos. Con esto se consigue eliminar ruido.
  matriz_tfidf_train <- dfm_trim(x = matriz_tfidf_train, min_docfreq = 5)
  
  # Conversión de los valores de la matriz a tf-idf
  matriz_tfidf_train <- dfm_tfidf(matriz_tfidf_train, scheme_tf = "prop",scheme_df = "inverse")
  matriz_tfidf_train
  
  # Limpieza y tokenización de los documentos de test
  tweets_test$texto <- tweets_test$texto %>% map(.f = limpiar_tokenizar) %>%map(.f = paste, collapse = " ") %>% unlist()
  
  # Identificación de las dimensiones de la matriz de entrenamiento
  # Los objetos dm() son de clase S4, se accede a sus elementos mediante @
  dimensiones_matriz_train <- matriz_tfidf_train@Dimnames$features
  
  # Conversión de vector a diccionario pasando por lista
  dimensiones_matriz_train <- as.list(dimensiones_matriz_train)
  names(dimensiones_matriz_train) <- unlist(dimensiones_matriz_train)
  dimensiones_matriz_train <- dictionary(dimensiones_matriz_train)
  dimensiones_matriz_train
  
  # Proyección de los documentos de test
  matriz_tfidf_test<-dfm(x = tweets_test$texto, dictionary = dimensiones_matriz_train)
  matriz_tfidf_test<-dfm_tfidf(matriz_tfidf_test, scheme_tf="prop",scheme_df = "inverse")
  
  # Como modelo de predicción se emplea una Máquina de soporte vectorial (SVM). 
  # Este método de aprendizaje estadístico suele dar buenos resultados en clasificación.
  # Ajuste del modelo
  modelo_svm <- svm(matriz_tfidf_train, y = as.factor(tweets_train$autor),
                    kernel = "linear", cost = 5, scale = TRUE, type = "C-classification")
  modelo_svm
  
  # Predicciones
  predicciones <- predict(object = modelo_svm, newdata = matriz_tfidf_test)
  
  # Error de predicción
  # Matriz de confusión
  table(observado = tweets_test$autor, predicho = predicciones)
  
  # Error de clasificación
  clasificaciones_erroneas <- sum(tweets_test$autor != predicciones)
  error <- 100 * mean(tweets_test$autor != predicciones)
  paste("Numero de clasificaciones incorrectas =", clasificaciones_erroneas)
  paste("Porcentaje de error =", round(error,2), "%")
  
  
  # Optimización de hiperparámetros
  # El método de SVM lineal tiene un único hiperparámetro C 
  # que establece la penalización por clasificación incorrecta, 
  # regulando así el balance entre bias y varianza. 
  # Al tratarse de un hiperparámetro, su valor óptimo 
  # no se aprende en el proceso de entrenamiento, 
  # para estimarlo hay que recurrir a validación cruzada.
  set.seed(369)
  svm_cv <- tune("svm", train.x = matriz_tfidf_train,
                 train.y = as.factor(tweets_train$autor),
                 kernel = "linear", ranges = list(cost = c(0.1, 0.5, 1, 2.5, 5)))
  summary(svm_cv)
  ggplot(data = svm_cv$performances, aes(x = cost, y = error)) +
    geom_line() +
    geom_point() +
    geom_errorbar(aes(ymin = error - dispersion, ymax = error + dispersion)) +
    theme_bw()
  
  #1) En este ejercicio se emplea la clasificación positivo/negativo proporcionada por el diccionario
  #bing.
  sentimientos <- get_sentiments(lexicon = "bing")
  head(sentimientos)
  sentimientos <- sentimientos %>% mutate(valor = if_else(sentiment == "negative", -1, 1))
  # sentimientos
  
  # Sentimiento promedio de cada tweet. Al disponer de los datos en formato tidy (una palabra por fila)
  # ,mediante un inner join se añade a cada palabra su sentimiento y se filtran automáticamente todas
  # aquellas palabras para las que no hay información disponible.
  tweets_sent <- inner_join(x = tweets_tidy, y = sentimientos, by = c("token" = "word"))
  tweets_sent
  
  #PRÁCTICA 13: ANÁLISIS DE SENTIMIENTOS
  # Una forma de analizar el sentimiento de un de un texto es considerando su sentimiento como 
  # la suma de los sentimientos de cada una de las palabras que lo forman. Esta forma consigue un buen
  # equilibro entre complejidad y resultados.
  
  download.file("https://raw.githubusercontent.com/jboscomendoza/rpubs/master/sentimientos_afinn/lexico_afinn.en.es.csv",
                "lexico_afinn.en.es.csv")
  sentimientos <- read.csv("lexico_afinn.en.es.csv", stringsAsFactors = F, fileEncoding = "latin1") %>% 
    tbl_df()
  sentimientos
  
  # 1) En este ejercicio se emplea la clasificación positivo/negativo proporcionada por el 
  # diccionario bing.
  
  sentimientos <- get_sentiments(lexicon = "bing")
  #head(sentimientos)
  
  sentimientos <- sentimientos %>% mutate(valor = if_else(sentiment == "negative", -1, 1))
  sentimientos
  
  tweets_sent <- inner_join(x = tweets_tidy, y = sentimientos, by = c("token" = "word"))
  tweets_sent
  
  
  #RETORNAR ESTA SUMA
  # Se suman los sentimientos de las palabras que forman 
  # cada tweet.
  tweets_sent %>% group_by(autor, tweet_id) %>%
    summarise(sentimiento_promedio = sum(valor)) %>%
    head()
  
  # Porcentaje de tweets positivos, 
  # negativos y neutros por autor
  tweets_sent %>% group_by(autor, tweet_id) %>%
    summarise(sentimiento_promedio = sum(valor)) %>%
    group_by(autor) %>%
    summarise(positivos = 100 * sum(sentimiento_promedio > 0) / n(),
              neutros = 100 * sum(sentimiento_promedio == 0) / n(),
              negativos = 100 * sum(sentimiento_promedio < 0) / n())
  
  # Palabras positivas y negativas más usadas 
  # por cada uno de ellos 
  # A la columna Puntuacion se le asigna la palabra
  # positivas o negativa dependiendo de su valor,
  # esto se hace para facilitas la graficacion
  tweets_sent_2 <- tweets_sent
  tweets_sent_2 <-
    tweets_sent_2 %>%
    mutate(valor = ifelse(valor > 0, "positive", "negative"))
  
  
  
  #COMPARACIÓN DE SENTIMIENTOS, IGUAL RETORNAR
  tweets_sent %>% group_by(autor, tweet_id) %>%
    summarise(sentimiento_promedio = sum(valor)) %>%
    group_by(autor) %>%
    summarise(positivos = 100*sum(sentimiento_promedio > 0) / n(),
              neutros = 100*sum(sentimiento_promedio == 0) / n(),
              negativos = 100*sum(sentimiento_promedio  < 0) / n()) %>%
    ungroup() %>%
    gather(key = "sentimiento", value = "valor", -autor) %>%
    ggplot(aes(x = autor, y = valor, fill = sentimiento)) + 
    geom_col(position = "dodge", color = "black") + coord_flip() +
    theme_bw()
  
  tweets_sent %>% mutate(anyo = year(fecha),
                         mes = month(fecha),
                         anyo_mes = ymd(paste(anyo, mes, sep="-"),truncated=2)) %>%
    group_by(autor, anyo_mes) %>%
    summarise(sentimiento = mean(valor)) %>%
    ungroup() %>%
    ggplot(aes(x = anyo_mes, y = sentimiento, color = autor)) +
    geom_point() + 
    geom_smooth() + 
    labs(x = "fecha de publicación") +
    facet_wrap(~ autor, ncol = 1) +
    theme_bw() +
    theme(legend.position = "none")
  
  # Quitamos “no” de nuestras palabras.
  # Es una palabra muy comun en inglés que no necesariamente implica un sentimiento negativo.
  # Es la palabra negativa más frecuente entre los candidatos, por lo que podría sesgar
  # nuestros resultados.
  
  tweets_sent_2 <-
    tweets_sent_2 %>%
    filter(token != "no")
  
  
  #RETORNAR
  # Palabras positivas:
  tweets_sent_2 %>%
    filter(sentiment == "positive") %>%
    group_by(autor) %>%
    count(token, sort = T) %>%
    top_n(n = 10, wt = n) %>%
    ggplot() +
    aes(token, n, fill = autor) +
    geom_col() +
    facet_wrap("autor", scales = "free") +
    scale_y_continuous(expand = c(0, 0)) +
    coord_flip() +
    labs(title = "Positive") +
    tema_graf
  
  # Palabras negativas:
  tweets_sent_2 %>%
    filter(sentiment == "negative") %>%
    group_by(autor) %>%
    count(token, sort = T) %>%
    top_n(n = 10, wt = n) %>%
    ggplot() +
    aes(token, n, fill = autor) +
    geom_col() +
    facet_wrap("autor", scales = "free") +
    scale_y_continuous(expand = c(0, 0)) +
    coord_flip() +
    labs(title = "Negative") +
    tema_graf
  
  # Como deseamos observar tendencias, vamos a obtener la media de sentimientos por día, 
  # usando group_by() y summarise() y asignamos los resultados a tuits_afinn_fecha
  tweets_sent_fecha <-
    tweets_sent
  
  tweets_sent_fecha <- tweets_sent_fecha %>% separate(fecha, into = c("Fecha", "Hora"), sep = " ")
  tweets_sent_fecha <- tweets_sent_fecha %>% separate(Fecha, into = c("Año", "Mes", "Dia"), sep = "-", remove = FALSE)
  
  # Como deseamos observar tendencias, vamos a obtener la media de sentimientos por día, usando 
  # group_by() y summarise() y asignamos los resultados a tuits_afinn_fecha
  tweets_sent_fecha_2 <-
    tweets_sent_fecha
  
  tweets_sent_fecha_2 <-
    tweets_sent_fecha_2 %>%
    group_by(tweet_id) %>%
    mutate(Suma = mean(valor)) %>%
    group_by(autor, Fecha) %>%
    summarise(Media = mean(valor))
  tweets_sent_fecha_2 <- tweets_sent_fecha_2 %>% mutate(Fecha = ymd(Fecha))
  
  
  
  # Veamos nuestros resultados con ggplot()
  tweets_sent_fecha_2 %>%
    ggplot() +
    aes(Fecha, Media, color = autor) +
    geom_line() +
    tema_graf +
    theme(legend.position = "top")  
  
  
  #RETORNAR
  # No nos dice mucho. Sin embargo, 
  # si separamos las líneas por candidato, 
  # usando facet_wrap(), será más fácil observar 
  # el las tendencias de los Candidatos.
  tweets_sent_fecha_2 %>%
    ggplot() +
    aes(Fecha, Media, color = autor) +
    geom_hline(yintercept = 0, alpha = .35) +
    geom_line() +
    facet_grid(autor~.) +
    tema_graf +
    theme(legend.position = "none")
  
  
  # PRÁCTICA 14
  #--------------------------------------------------- 
  # Word Clouds 
  
  
  df_grouped <- tweets_tidy %>% group_by(autor, token) %>% count(token) %>% 
    group_by(autor) %>% mutate(frecuencia = n / n()) %>% 
    arrange(autor, desc(frecuencia)) %>% nest() %>% 
    filter(autor == "JoeBiden")  
  
  options(warn=-1)
  
  walk2(.x = "JoeBiden", .y = df_grouped$data, .f = wordcloud_custom) 
  walk2(.x = "BernieSanders", .y = df_grouped$data, .f = wordcloud_custom)
  
  # Sentimientos: 
  tweets_sent <- inner_join(x = tweets_tidy, y = sentimientos, by = c("token" = "word")) 
  tweets_sent 
  tweets_sent %>% group_by(autor, tweet_id) %>% 
    summarise(sentimiento_promedio = sum(valor)) %>% 
    head() 
  tweets_sent %>% group_by(autor, tweet_id) %>% 
    summarise(sentimiento_promedio = sum(valor)) %>% 
    group_by(autor) %>% 
    summarise(positivos = 100 * sum(sentimiento_promedio > 0) / n(), 
              neutros = 100 * sum(sentimiento_promedio == 0) / n(),negativos = 100 * sum(sentimiento_promedio  < 0) / n()) 
  
  tweets_sent_2 <- tweets_sent 
  tweets_sent_2 <- 
    tweets_sent_2 %>% 
    mutate(Puntuacion = ifelse(valor > 0, "Positiva", "Negativa")) 
  tweets_sent_2 <- 
    tweets_sent_2 %>% 
    filter(token != "no")
  
  
  # Word Cloud: 
  tweets_sent_2 %>% group_by(autor) %>% 
    filter(autor == "JoeBiden") %>% 
    count(token, valor, sort = TRUE) %>% 
    acast(token ~ valor, value.var = "n", fill = 0) %>% 
    comparison.cloud(colors = c("indianred1", "lightseagreen"), 
                     max.words = 100)
  
  tweets_sent_2 %>% group_by(autor) %>% 
    filter(autor == "BernieSanders") %>% 
    count(token, Puntuacion, sort = TRUE) %>% 
    acast(token ~ Puntuacion, value.var = "n", fill = 0) %>% 
    comparison.cloud(colors = c("indianred1", "lightseagreen"), max.words = 100)
  
}




limpiar_tokenizar <- function(texto){
  nuevo_texto <- tolower(texto) # 1
  # de cualquier cosa que no sea un espacio)
  nuevo_texto <- str_replace_all(nuevo_texto,"http\\S*", "") # 2
  nuevo_texto <- str_replace_all(nuevo_texto,"[[:punct:]]", " ") # 3
  nuevo_texto <- str_replace_all(nuevo_texto,"[[:digit:]]", " ") # 4
  nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ") # 5
  nuevo_texto <- str_replace_all(nuevo_texto, "[\\U0001f100-\\U0001ffff]", " ")#emojis
  nuevo_texto <- str_replace_all(nuevo_texto, "[\\u200D-\\u2fff]", " ")#emojis
  nuevo_texto <- str_split(nuevo_texto, " ")[[1]] #6
  nuevo_texto <- keep(.x = nuevo_texto, .p = function(x){str_length(x) > 1})
  return(nuevo_texto)
}




limpiar <- function(texto){
  # Se convierte todo el texto a minúsculas
  nuevo_texto <- tolower(texto)
  # Eliminación de páginas web (palabras que empiezan por "http." seguidas 
  # de cualquier cosa que no sea un espacio)
  nuevo_texto <- str_replace_all(nuevo_texto,"http\\S*", "")
  # Eliminación de signos de puntuación
  nuevo_texto <- str_replace_all(nuevo_texto,"[[:punct:]]", " ")
  # Eliminación de números
  nuevo_texto <- str_replace_all(nuevo_texto,"[[:digit:]]", " ")
  # Eliminación de espacios en blanco múltiples
  nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ")
  return(nuevo_texto)
}

wordcloud_custom <- function(grupo, df){ 
  print(grupo) 
  wordcloud(words = df$token, freq = df$frecuencia, 
            max.words = 400, random.order = FALSE, rot.per = 0.35, 
            colors = brewer.pal(8, "Dark2")) 
}
